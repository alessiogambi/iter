ITER
====

An extensible tool for search-based system testing of Cloud applications.
  
This tool implements an evolutionary process that is specifically designed to incrementally build test suites for system testing of applications that run in clouds under the *Infrastructure-as-a-Service (IaaS)* paradigm.
ITER leverages a powerful test driver, called AUToCLES[1], that automates the deployment of cloud-based applications that are formally specified through a *service manifest* file, the generation of the workload that is specified by an Apache JMeter[2] *test plan*, and the collection of the data inside a private *memcached* service.
AUToCLES exposes a Web user interface and a REST API that ITER binds to schedule test executions and to retrieve published test execution data.

The theory underlying ITER is described in some research papers (see the References Section at the end of this file) and is briefly summarized in the next Section.

The architecture of the tool, its main features, the intended users and the available extensions are described in a paper submitted to the Formal Demo track of the International Conference on Software Engineering (ICSE'14)[3]. The next sections summarize these aspects, and we remand to the code for a detailed understanding of the tool's implementation.

A short video showcase how the tool works, and it can be found <a href="http://www.youtube.com/watch?feature=player_embedded&v=YOUTUBE_VIDEO_ID_HERE " target="_blank">on YouTube</a>.

# Summary of the Main Theoretical Aspects

System-testing is a wide topic, and it can be implemented in a multitude of ways.
We decided to adopt an automatic and iterative approach to generate and execute test cases.
We focus on a particular for of test cases that is suitable to test cloud-application *elasticity*.
In other words, we are mainly focused to test if the cloud-application are able to adapt to the input workload as their developer expect.
  
Test cases are composed of the following ingredients:  
1) A formal description of the system under test that specifies all the settings and data to automatically deploy the whole SUT. For example, the type and flavor of the virtual machines that comprise the elastic application, the software components that run on each of the as well as their customization and start-up parameters.  

2) A formal description of possible user sessions (sequences of requests issued from the clients). We assume the availability of a test dataset to be used during the test execution or the ability of the clients to generate data on the fly (for example, randomly).

3) A formal specification of workload that we will use to stress the application. We encode this as time depend functions that compute at each time the amount of user sessions (or clients) that are concurrently active.

4) A set of assertions over the collected test execution data. In particular, the assertions encodes conditions over the time varying variables that are collected during the run by the SUT and the platform. For this reason, we assume that SUT can monitor and expose the needed variables.

The test suite is then generated by instantiating any combination of these elements; however, in practice we fix 1, 2 and 4, and only let 3 to vary.
In this way, we generate easily different test cases that target the very same deployment for the SUT by applying different patterns of workload.

## A Simple Example

Imagine that we need to test an elastic n-tier Web application that for example implement and auction site.
The application is composed of various components that are run by different virtual machines. For example, the load balancer and the monitoring component run in a medium instance, the web server and the application server in a small instance and the database in a large instance. Web and application servers can dynamically added and removed. An additional small instance runs the control logic that decides when to add and remove the instances of these machines. All these information (and something more) are stored inside the service manifest (1).

To stress the application we define two types of user sessions, namely *buyers* and *sellers*, that defined (possibly randomized) sequences of users request. For example, `login`, `search product`, `bid`, `buy`, and so on. The logic of user sessions is captured inside a JMeter test plan, a widely known workload generator (2).

We decide to mimic periodic trends for both the types of user sessions, so we model the amount of concurrent active users by two sine functions.
For example, we define that the amount of buyer sessions behaves according to a sine wave that passes from 0 to 100 users over a period of 30 mins, starting from 0. Similarly we define that seller sessions start from 10 and go up to 50 over a period of 1 hour. These workload are encoded in simple trace files that are obtained by evaluating the wave functions in each second (3).

Finally, we define two assertions the first is one the number of web/applications servers that must never become smaller than one, and the second is about the average response time of the user requests that should never exceed 2 seconds. Assuming the availability of these data we encode the assertion logic inside Java code (4).

# Target Users

This tool targets all the persons that should/must/would evaluate the quality of software of cloud-based applications. In particular, we target software developers and testers.

# Usage Scenarios

## Main Scenario: Test case generation

The main scenario is the one of incremental generation of test suite. A tester prepares the service manifest, user sessions definition and the assertions, and she configures the initial test case generation and evolution policy. Then the tool is started with additional configurations that define for example the space of the search, the amount of parallelism for executing the tests and test report file that must be produced. The tool starts by creating and executing some initial (random) tests, then evolves the test suite, and it runs until it completed the search or an exception is generated. During the run, partial results and executed test cases are stored in the test report file. At the end of the run, the test report contains the test suites and all the data about the test cases ran.

## Bootstrap

System-testing is an activity that may take a long time, so starting a new search every time may be tedious (note that we are not talking about regression testing!). Furthermore, many times the evolution process depends on previously collected data, and if those are available the process can be bootstrapped.
ITER allows search to be bootstrapped by providing a test report as input. When ITER uses the bootstrap, it does not repeat the test execution but it uses the available data to evaluate from all the provided assertions. This process speeds up the search algorithm, but it can also be used to run different search algorithms by starting all of them form the same state.

## Dry Run

ITER's default search policy is to stop without making any evolutionary step. This can be used in combination with the bootstrap when new user assertions are defined. In this case, the new assertions can be evaluated against an input dataset to produce an updated test report without the need of re-executing the tests.

## Regression

ITER can be configured also to run an input set of tests. By feeding the tool with the previously executed tests, it indeed implements a for of regression test.
**Note:** Some of the features that are required to  implement this scenario are currently under-development.

# Tool Features

The most important features of the tool are listed and briefly described below.

- Bootstrap  
- Generation of the 
- Parallel Test Execution  
- Data Collection  
- Assertion framework  
- Test Report Generation  
- Evolutionary Policy
	Available:
	- Pure Random
	- Plasticity Driven Search
	
# Tool Architecture

# Other Implementation Details

# Code Download

Maven dependency form the Infosys repository

# References

A. Gambi, W. Hummer, H.L. Truong, and S. Dustdar.
*Testing Elastic Computing Systems*.
Submitted to IEEE Internet Computing, 2013

A. Gambi, W. Hummer, and S. Dustdar.
*Automated Testing of Cloud-Based Elastic Systems with AUToCLES*.
In Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering (ASE),
November 11-15, 2013, Palo Alto, California (USA)

A. Gambi, A. Filieri, and S. Dustdar.
*Iterative Test Suites Refinement for Elastic Computing Systems*.
In Proceedings of the joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE), August 18-26, 2013, Saint Petersburg, Russia

A. Gambi, W. Hummer, and S. Dustdar.
*Testing Elastic Systems with Surrogate Models*.
In Proceedings of the International Workshop on Combining Modelling and Search-Based Software Engineering (CMSBSE) (co-located with ICSE'13),
May 20, 2013, San Francisco, California, USA  

[1] http://dsg.tuwien.ac.at/autocles/
[2] http://jmeter.apache.org/
[3] http://2014.icse-conferences.org/demo